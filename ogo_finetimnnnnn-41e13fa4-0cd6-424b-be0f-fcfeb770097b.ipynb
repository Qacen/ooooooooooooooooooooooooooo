{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2FDVspYdIl4"
   },
   "source": [
    "**Homework main task**: overcome Mistral hallucinations in chat mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nALviN0PetMT"
   },
   "source": [
    "### Plan:\n",
    "\n",
    "- Test the pre-trained model as an online store assistant.\n",
    "- Completely train the model using the LoRA method on data from chatbot communication with clients on various products.\n",
    "- Test the completed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8mkjNUdebSE"
   },
   "source": [
    ":You need to fill in all the blankes in the code (there are 7 spaces in total)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK8WVBwzhy6d"
   },
   "source": [
    "**Note!** Please safe all cells outputs, otherwise the task will not be accepted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RMBKcUhhl7i"
   },
   "source": [
    "# Fine-tune Mistral 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141150,
     "status": "ok",
     "timestamp": 1743681378232,
     "user": {
      "displayName": "Ильсеяр Алимова",
      "userId": "18086433180291504366"
     },
     "user_tz": -180
    },
    "id": "tYLWI6YDsuXU",
    "outputId": "91365687-5410-4749-8d04-863c555ae15e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.11.3 requires packaging<24, but you have packaging 24.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft transformers bitsandbytes accelerate trl datasets -U -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u__5Qp_frMGA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJqIm1QqhxmB"
   },
   "source": [
    "Let's first test how the Mistral 7b, pre-trained in Russian (the original model was trained only in English), will cope with this task. Let the model play the role of an assistant for an online smartphone store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hYmD4GYPs6EW"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "SYSTEM_PROMPT = \"\"\"Ты – чат-бот технической поддержки Xiaomi Store, который помогает клиенту выбрать наиболее подходящий для него смартфон. Опираясь на описание смартфонов, помоги клиенту выбрать наиболее подходящий для него смартфон. Если ответа на вопрос клиента нет в приведенном описании, ответь \"У меня не достаточно информации для ответа на ваш вопрос. Обратитесь пожалуйста к менеджеру в telegram\".\n",
    "Описание смартфонов из интернет-магазина сотовой связи Xiaomi Store:\n",
    "1. Смартфон Xiaomi Redmi Note 10 Pro: Этот смартфон оснащен дисплеем Super AMOLED с разрешением 1080 x 2400 пикселей, что обеспечивает четкую и яркую картинку. Он также имеет камеру на 64 Мп с возможностью записи видео в 4K. Процессор Snapdragon 732G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
    "2. Смартфон Xiaomi Mi 11 Lite: Этот смартфон имеет ультратонкий и легкий дизайн, который удобно держать в руке. Он оснащен 6.55-дюймовым AMOLED-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп способна делать качественные фотографии, а быстрый процессор Qualcomm Snapdragon 732G позволяет работать с приложениями плавно.\n",
    "3. Смартфон Xiaomi Redmi 9T: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей, который обеспечивает реалистичную цветопередачу. Камера на 48 Мп делает четкие фото, а процессор Snapdragon 662 обеспечивает быструю работу. Аккумулятор на 6000 мАч позволяет использовать устройство долгое время без подзарядки.\n",
    "4. Смартфон Xiaomi Mi 10T Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 108 Мп, что позволяет делать потрясающие фотографии. Процессор Qualcomm Snapdragon 865 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\n",
    "5. Смартфон Xiaomi Redmi Note 9 Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп с поддержкой искусственного интеллекта позволяет делать яркие и четкие фотографии. Процессор Snapdragon 720G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
    "6. Смартфон Xiaomi Mi 10T Lite: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 64 Мп, что позволяет делать яркие и детальные фотографии. Процессор Qualcomm Snapdragon 750G ускоряет работу с приложениями, а аккумулятор на 4820 мАч обеспечивает долгую автономность.\n",
    "7. Смартфон Xiaomi Redmi Note 8 Pro: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 64 Мп делает четкие и яркие фото, а процессор MediaTek Helio G90T обеспечивает плавную работу. Аккумулятор на 4500 мАч достаточно емкий для долгого использования.\n",
    "8. Смартфон Xiaomi Mi 10 Pro: Этот смартфон оснащен 6.67-дюймовым AMOLED-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 108 Мп и оптическая стабилизация изображения позволяют делать высококачественные фотографии. Процессор Snapdragon 865 ускоряет работу с приложениями, а аккумулятор на 4500 мАч достаточно емкий.\n",
    "9. Смартфон Xiaomi Redmi 9C: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 720 x 1600 пикселей. Он имеет камеру на 13 Мп, которая делает четкие фото в хороших условиях освещения. Процессор MediaTek Helio G35 обеспечивает достаточную производительность для базовых задач, а аккумулятор на 5000 мАч обеспечивает долгое время работы.\n",
    "10. Смартфон Xiaomi Mi 11 Ultra: Этот смартфон оснащен 6.81-дюймовым AMOLED-дисплеем с разрешением 1440 x 3200 пикселей. Он имеет камеру на 50 Мп, а также вспомогательные камеры для различных эффектов съемки. Процессор Snapdragon 888 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D6ceOj-RtDpH"
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=MESSAGE_TEMPLATE,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        response_template=RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        # Инициализация класса Conversation с шаблонами сообщений и системным запросом.\n",
    "        # message_template - шаблон для каждого сообщения (например, форматирование текста).\n",
    "        # system_prompt - начальный запрос, задающий контекст разговора.\n",
    "        # response_template - шаблон для ответа, который будет добавлен в конце текста.\n",
    "\n",
    "        self.message_template = message_template  # Сохранение шаблона сообщений.\n",
    "        self.response_template = response_template  # Сохранение шаблона ответа.\n",
    "        # Инициализация списка сообщений с первым сообщением от системы, определяющим контекст беседы.\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        # Метод для добавления сообщения пользователя в список сообщений.\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        # Метод для добавления сообщения бота в список сообщений.\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        # Метод для формирования финального текста запроса, который будет отправлен для генерации ответа.\n",
    "        final_text = \"\"  # Инициализация переменной для хранения окончательного текста.\n",
    "        for message in self.messages:\n",
    "            # Форматирование каждого сообщения в соответствии с message_template.\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text  # Добавление отформатированного текста к итоговому.\n",
    "\n",
    "        final_text += RESPONSE_TEMPLATE  # Добавление шаблона ответа в конце текста.\n",
    "        return final_text.strip()  # Возвращение итогового текста без лишних пробелов в начале и в конце.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0Tfx9wT3tIGp"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    # Функция для генерации текста на основе заданного промпта с использованием модели и токенизатора.\n",
    "\n",
    "    # Токенизация промпта: превращаем текстовый промпт в числовые представления (токены),\n",
    "    # которые понимает модель. Параметр return_tensors=\"pt\" означает, что данные\n",
    "    # будут возвращены в формате PyTorch тензоров. add_special_tokens=False указывает,\n",
    "    # что специальные токены (например, [CLS], [SEP]) не будут добавлены.\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    # Переносим данные на устройство, на котором работает модель (например, GPU или CPU).\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    # Генерация выходных идентификаторов (токенов) с использованием модели.\n",
    "    # Параметр generation_config задает конфигурацию генерации, например,\n",
    "    # максимальную длину ответа, температуру и т.д.\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "\n",
    "    # Обрезаем выходные идентификаторы, чтобы исключить исходные токены промпта.\n",
    "    # Таким образом, оставляем только сгенерированный моделью ответ.\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "\n",
    "    # Декодируем токены обратно в текст, исключая специальные токены, такие как [PAD] или [SEP].\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Возвращаем итоговый сгенерированный текст, убирая лишние пробелы в начале и в конце.\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IcfEtzF2tSBI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"no_repeat_ngram_size\": 15,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NyManLv7tUFv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32002, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем конфигурацию модели PeftConfig из предобученной модели, используя ее имя.\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Загружаем предобученную модель для задачи автозавершения текста (Causal Language Modeling).\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "\n",
    "# Переводим модель в режим оценки, чтобы отключить функции, используемые только при обучении,\n",
    "# такие как dropout, и настроить модель на генерацию предсказаний.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_HlOjMwMtW04"
   },
   "outputs": [],
   "source": [
    "conversation = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmGXnnEatZvp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Клиент:  4к планшет для рисования\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чат-бот: К сожалению, в Xiaomi Store не продаются планшеты для рисования. Однако, могу предложить вам несколько альтернативных вариантов:\n",
      "\n",
      "1. Xiaomi Pad 5 - это планшет с дисплеем 11-дюймов, который подходит для рисования благодаря своей тонкой и удобной форме. Он оснащен процессором Qualcomm Snapdragon 860, что обеспечивает быструю работу.\n",
      "\n",
      "2. Xiaomi Pad 5 Pro - это более мощный вариант с процессором Qualcomm Snapdragon 870, который обеспечивает еще большую производительность. Он также оснащен дисплеем 12.4-дюймов, что делает его идеальным для рисования.\n",
      "\n",
      "3. Xiaomi Pad 5 Kids Edition - это специальная версия планшета, которая предназначена для детей. Он оснащен дисплеем 10.1-дюймов, а также включает в себя специальное программное обеспечение для безопасного использования планшетом детьми.\n",
      "\n",
      "Все эти планшеты поддерживают стилусы, которые можно использовать для рисования и создания различных проектов.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_uttr = input(\"Клиент: \")\n",
    "    conversation.add_user_message(user_uttr)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    output = output.split(\"bot\")[0].strip()\n",
    "    print(f\"Чат-бот: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9VYwBwkjP8A"
   },
   "source": [
    "Let's start a conversation. Try asking to tell about tablets that have 4K video quality. We see that the bot starts to invent non-existent information, that is, it starts to hallucinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbFbuumkju4_"
   },
   "source": [
    "Downloading data for additional fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oHn7o0rn1Qjg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-11 02:22:38--  https://docs.google.com/uc?export=download&id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT\n",
      "Resolving docs.google.com (docs.google.com)... 64.233.162.194, 2a00:1450:4010:c05::c2\n",
      "connected. to docs.google.com (docs.google.com)|64.233.162.194|:443... \n",
      "303 See Othersent, awaiting response... \n",
      "Location: https://drive.usercontent.google.com/download?id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT&export=download [following]\n",
      "--2025-04-11 02:22:39--  https://drive.usercontent.google.com/download?id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT&export=download\n",
      "173.194.221.132, 2a00:1450:4001:81c::2001rive.usercontent.google.com)... \n",
      "connected. to drive.usercontent.google.com (drive.usercontent.google.com)|173.194.221.132|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 70497894 (67M) [application/octet-stream]\n",
      "Saving to: ‘company_cases.json’\n",
      "\n",
      "company_cases.json  100%[===================>]  67.23M  51.1MB/s    in 1.3s    \n",
      "\n",
      "2025-04-11 02:22:46 (51.1 MB/s) - ‘company_cases.json’ saved [70497894/70497894]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT' -O company_cases.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.local/lib/python3.11/site-packages (4.51.2)\n",
      "Requirement already satisfied: filelock in /home/user/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.local/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/conda/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/user/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/user/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fX-eE4Dx1pAF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, GenerationConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f6t0HfEOu_b0"
   },
   "outputs": [],
   "source": [
    "with open(\"company_cases.json\", 'r') as inp:\n",
    "    raw_dataset = json.load(inp)\n",
    "\n",
    "train_raw_dataset, test_raw_dataset = train_test_split(raw_dataset)\n",
    "train_test_raw_dataset = {\"train\": train_raw_dataset, \"test\": test_raw_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeoNwnUcj9Nt"
   },
   "source": [
    "Let's convert the dataset into the format used in the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "auxOuusLvfae"
   },
   "outputs": [],
   "source": [
    "raw_dataset_dict = {}\n",
    "for data_type in [\"train\", \"test\"]:\n",
    "    raw_dataset_dict[data_type] = {\n",
    "        \"instruction\": [element['instruction'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"personality\": [element['personality'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"context\": [element['context'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"dialog_start_line\": [element['instruction'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"dialog\": [element['dialog'] for element in train_test_raw_dataset[data_type]]\n",
    "    }\n",
    "\n",
    "train_dataset = Dataset.from_dict(raw_dataset_dict[\"train\"])\n",
    "test_dataset = Dataset.from_dict(raw_dataset_dict[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el_YrV5fj4Ez"
   },
   "source": [
    "Function for formatting the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "buvCQ5jrvi7A"
   },
   "outputs": [],
   "source": [
    "def formatting_prompt_func(example):\n",
    "    prompt = f\"<s>system\\n{example['instruction']}\"\n",
    "    if example['personality']:\n",
    "        prompt += f\"\\n{example['personality']}\"\n",
    "    prompt += f\"\\n{example['context']}\"\n",
    "    if example['dialog_start_line']:\n",
    "        prompt += f\"\\n{example['dialog_start_line']}\"\n",
    "    prompt += \"</s>\"\n",
    "    for element in example['dialog']:\n",
    "        prompt += f\"<s>{element['role']}\\n{element['content']}</s>\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjA-Xy5IkLug"
   },
   "source": [
    "For additional training, we will take not the ready Russian-language Mistral, but the pre-trained Mistral on the Open-Orca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZcE7QObvvk3z"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDDDT6cgkJKp"
   },
   "source": [
    "Let's make the gradient during training flow only through the tokens of the last replica of the chatbot, and let's make the labels for the remaining tokens equal to -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cb6Jmqr4vmm4"
   },
   "outputs": [],
   "source": [
    "response_template = \"bot\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Sb3ptnSLvo4v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.86s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Open-Orca/Mistral-7B-OpenOrca\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yV8Xfkhkg_3"
   },
   "source": [
    "Initializing LoraConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9DFYOoXovs1H"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnpWIfYykmv6"
   },
   "source": [
    "Setting parameters for additional training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PBp2aMHKvvT2"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"/home/jovyan/checkpoints/\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=25,\n",
    "    save_strategy='steps',\n",
    "    save_steps=25,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=10,\n",
    "    report_to=\"none\",\n",
    "    max_length=1024,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqruUzVThaMk"
   },
   "source": [
    "Disable tokenizer parallelism to avoid deadlocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K1f8cTn-hZYk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il3hvsNrkrQy"
   },
   "source": [
    "Starting fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 8354/8354 [00:01<00:00, 4642.44 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 8354/8354 [00:00<00:00, 10253.06 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 8354/8354 [00:00<00:00, 13326.21 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 8354/8354 [00:20<00:00, 405.88 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 8354/8354 [00:00<00:00, 13838.86 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 2785/2785 [00:00<00:00, 4430.63 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 2785/2785 [00:00<00:00, 13187.12 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 2785/2785 [00:00<00:00, 12412.85 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2785/2785 [00:06<00:00, 409.93 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 2785/2785 [00:00<00:00, 15675.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='417' max='417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [417/417 1:27:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.799800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.617800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.603700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.571800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.563100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.569800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.586000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=417, training_loss=0.6227154309120632, metrics={'train_runtime': 5243.6189, 'train_samples_per_second': 1.592, 'train_steps_per_second': 0.0795, 'total_flos': 3.634812051900928e+15, 'train_loss': 0.6227154309120632})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    formatting_func=formatting_prompt_func,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tARaNoT7kwLN"
   },
   "source": [
    "Test the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, GenerationConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2_IdbgOmA1U"
   },
   "source": [
    "Loading the adapter checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eNBbYnW0QpdG"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "ADAPTER_MODEL = \"checkpoints/checkpoint-417\"\n",
    "MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "SYSTEM_PROMPT = \"\"\"Ты – чат-бот технической поддержки Xiaomi Store, который помогает клиенту выбрать наиболее подходящий для него смартфон. Опираясь на описание смартфонов, помоги клиенту выбрать наиболее подходящий для него смартфон. Если ответа на вопрос клиента нет в приведенном описании, ответь \"У меня не достаточно информации для ответа на ваш вопрос. Обратитесь пожалуйста к менеджеру в telegram\".{personality}\n",
    "Описание смартфонов из интернет-магазина сотовой связи Xiaomi Store:\n",
    "1. Смартфон Xiaomi Redmi Note 10 Pro: Этот смартфон оснащен дисплеем Super AMOLED с разрешением 1080 x 2400 пикселей, что обеспечивает четкую и яркую картинку. Он также имеет камеру на 64 Мп с возможностью записи видео в 4K. Процессор Snapdragon 732G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
    "2. Смартфон Xiaomi Mi 11 Lite: Этот смартфон имеет ультратонкий и легкий дизайн, который удобно держать в руке. Он оснащен 6.55-дюймовым AMOLED-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп способна делать качественные фотографии, а быстрый процессор Qualcomm Snapdragon 732G позволяет работать с приложениями плавно.\n",
    "3. Смартфон Xiaomi Redmi 9T: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей, который обеспечивает реалистичную цветопередачу. Камера на 48 Мп делает четкие фото, а процессор Snapdragon 662 обеспечивает быструю работу. Аккумулятор на 6000 мАч позволяет использовать устройство долгое время без подзарядки.\n",
    "4. Смартфон Xiaomi Mi 10T Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 108 Мп, что позволяет делать потрясающие фотографии. Процессор Qualcomm Snapdragon 865 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\n",
    "5. Смартфон Xiaomi Redmi Note 9 Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп с поддержкой искусственного интеллекта позволяет делать яркие и четкие фотографии. Процессор Snapdragon 720G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
    "6. Смартфон Xiaomi Mi 10T Lite: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 64 Мп, что позволяет делать яркие и детальные фотографии. Процессор Qualcomm Snapdragon 750G ускоряет работу с приложениями, а аккумулятор на 4820 мАч обеспечивает долгую автономность.\n",
    "7. Смартфон Xiaomi Redmi Note 8 Pro: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 64 Мп делает четкие и яркие фото, а процессор MediaTek Helio G90T обеспечивает плавную работу. Аккумулятор на 4500 мАч достаточно емкий для долгого использования.\n",
    "8. Смартфон Xiaomi Mi 10 Pro: Этот смартфон оснащен 6.67-дюймовым AMOLED-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 108 Мп и оптическая стабилизация изображения позволяют делать высококачественные фотографии. Процессор Snapdragon 865 ускоряет работу с приложениями, а аккумулятор на 4500 мАч достаточно емкий.\n",
    "9. Смартфон Xiaomi Redmi 9C: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 720 x 1600 пикселей. Он имеет камеру на 13 Мп, которая делает четкие фото в хороших условиях освещения. Процессор MediaTek Helio G35 обеспечивает достаточную производительность для базовых задач, а аккумулятор на 5000 мАч обеспечивает долгое время работы.\n",
    "10. Смартфон Xiaomi Mi 11 Ultra: Этот смартфон оснащен 6.81-дюймовым AMOLED-дисплеем с разрешением 1440 x 3200 пикселей. Он имеет камеру на 50 Мп, а также вспомогательные камеры для различных эффектов съемки. Процессор Snapdragon 888 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xftgOaEpQtnH"
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=MESSAGE_TEMPLATE,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        response_template=RESPONSE_TEMPLATE,\n",
    "        personality=None\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        if personality:\n",
    "            system_prompt = system_prompt.format(personality=personality)\n",
    "        else:\n",
    "            system_prompt = system_prompt.format(personality=\"\")\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += RESPONSE_TEMPLATE\n",
    "        return final_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EB4bh1U1QyIN"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "W7V0kJ2MQ1CL"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    pad_token_id=0,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p9wUZBSgQ64L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7y0XOd29RBG_"
   },
   "outputs": [],
   "source": [
    "conversation = Conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTVEBA7QmZgc"
   },
   "source": [
    "We start the dialogue again and ask about the tablets. Were you able to overcome the hallucinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEbsJX89RGOV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Клиент:  4к планшет для рисования\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Клиент: 4к планшет для рисования\n",
      "Чат-бот: К сожалению, у меня нет информации о планшетах для рисования в Xiaomi Store. Вам стоит обратиться к менеджеру в telegram.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_uttr = input(\"Клиент: \")\n",
    "    print(f\"Клиент: {user_uttr}\")\n",
    "    conversation.add_user_message(user_uttr)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    output = output.split(\"bot\")[0].replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "    print(f\"Чат-бот: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
