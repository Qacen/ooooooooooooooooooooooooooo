{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11483707,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11267031,"sourceType":"datasetVersion","datasetId":7042864}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torchmetrics import Accuracy, F1Score\n\ndf = pd.read_csv('/kaggle/input/qefasfas/train.tsv')\ndf['class'] = df['class'].astype(int) \n\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=43)\n\n\nclass TweetDataset(Dataset):\n    def __init__(self, texts, targets):\n        self.texts = texts\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return {'text': self.texts[idx], 'class': self.targets[idx]}\n\n\nclass Collator:\n    def __init__(self, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __call__(self, batch):\n        texts = [item['text'] for item in batch]\n        targets = [item['class'] for item in batch]\n        encoding = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'],\n            'attention_mask': encoding['attention_mask'],\n            'labels': torch.tensor(targets, dtype=torch.long)\n        }\n\nclass ClassificationModel(pl.LightningModule):\n    def __init__(self, model_name=\"deepvk/USER-base\", lr=2e-5):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.model = AutoModel.from_pretrained(model_name)\n        self.classifier = nn.Linear(self.model.config.hidden_size, 1)\n        \n        self.register_buffer('pos_weight', torch.tensor([5.0]))\n        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n\n        self.train_acc = Accuracy(task=\"binary\")\n        self.val_acc = Accuracy(task=\"binary\")\n        self.train_f1 = F1Score(task=\"binary\")\n        self.val_f1 = F1Score(task=\"binary\")\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        pooled = outputs.last_hidden_state.mean(dim=1)\n        logits = self.classifier(pooled)\n        return logits.squeeze()\n\n    def training_step(self, batch, batch_idx):\n        logits = self(batch['input_ids'], batch['attention_mask'])\n        loss = self.loss_fn(logits, batch['labels'].float())\n        preds = torch.sigmoid(logits) > 0.5\n        \n        self.train_acc(preds, batch['labels'])\n        self.train_f1(preds, batch['labels'])\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n\n    def on_train_epoch_end(self):\n        self.log('train_acc', self.train_acc.compute(), prog_bar=True)\n        self.log('train_f1', self.train_f1.compute(), prog_bar=True)\n        self.train_acc.reset()\n        self.train_f1.reset()\n\n    def validation_step(self, batch, batch_idx):\n        logits = self(batch['input_ids'], batch['attention_mask'])\n        loss = self.loss_fn(logits, batch['labels'].float())\n        preds = torch.sigmoid(logits) > 0.5\n        \n        self.val_acc(preds, batch['labels'])\n        self.val_f1(preds, batch['labels'])\n        self.log('val_loss', loss, prog_bar=True)\n        return loss\n\n    def on_validation_epoch_end(self):\n        self.log('val_acc', self.val_acc.compute(), prog_bar=True)\n        self.log('val_f1', self.val_f1.compute(), prog_bar=True)\n        self.val_acc.reset()\n        self.val_f1.reset()\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n\n\nmodel_name = \"deepvk/USER-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ncollator = Collator(tokenizer)\n\ntrain_loader = DataLoader(\n    TweetDataset(train_df['tweet'].tolist(), train_df['class'].tolist()),\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collator,\n    persistent_workers=True\n)\n\nval_loader = DataLoader(\n    TweetDataset(val_df['tweet'].tolist(), val_df['class'].tolist()),\n    batch_size=32,\n    num_workers=4,\n    collate_fn=collator,\n    persistent_workers=True\n)\n\n\ncheckpoint = pl.callbacks.ModelCheckpoint(\n    monitor='val_f1',\n    mode='max',\n    save_top_k=1,\n    filename='best-{epoch}-{val_f1:.2f}'\n)\n\n\ntrainer = pl.Trainer(\n    max_epochs=7,\n    accelerator='gpu',\n    devices=-1,  \n    strategy='ddp_notebook',\n    callbacks=[checkpoint],\n    enable_progress_bar=True,\n    log_every_n_steps=10\n)\n\nmodel = ClassificationModel()\ntrainer.fit(model, train_loader, val_loader)\n\nbest_model_path = checkpoint.best_model_path\nprint(f\"Best model saved at: {best_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T19:57:06.868745Z","iopub.execute_input":"2025-04-03T19:57:06.869082Z","iopub.status.idle":"2025-04-03T20:02:35.597193Z","shell.execute_reply.started":"2025-04-03T19:57:06.869050Z","shell.execute_reply":"2025-04-03T20:02:35.596372Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a556e1c457c645ef969c2b8c9f0f9fd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('train_f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Best model saved at: /kaggle/working/lightning_logs/version_26/checkpoints/best-epoch=3-val_f1=0.53.ckpt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\ndef predict_with_existing_model(model, data_loader, device='cuda', has_labels=True):\n    model.to(device)\n    model.eval()\n    \n    probabilities = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            logits = model(input_ids, attention_mask)\n            probas = torch.sigmoid(logits)\n            \n            probabilities.extend(probas.cpu().numpy())\n            if has_labels:\n                all_targets.extend(batch['labels'].cpu().numpy())\n    \n    # Подбор трешхолда если есть метки\n    if has_labels and len(all_targets) > 0:\n        best_thresh, best_f1 = find_optimal_threshold(all_targets, np.array(probabilities))\n        print(f\"Optimal threshold: {best_thresh:.4f} with F1: {best_f1:.4f}\")\n    else:\n        best_thresh = 0.015\n        print(\"Using default threshold 0.5\")\n    \n    predictions = (np.array(probabilities) >= best_thresh).astype(int)\n    return predictions, probabilities\n\ntest_df = pd.read_csv(\"/kaggle/input/qefasfas/test.tsv\")\ntest_dataset = TweetDataset(test_df['tweet'].tolist(), [0]*len(test_df))  \ntest_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collator)\n\ntest_predictions, test_probas = predict_with_existing_model(\n    model=model,\n    data_loader=test_loader,\n    device='cuda',\n    has_labels=False\n)\n\n\ntest_df['class'] = test_predictions\ntest_df['probability'] = test_probas\ntest_df[['id','class']].to_csv('predictions.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T20:06:15.703907Z","iopub.execute_input":"2025-04-03T20:06:15.704205Z","iopub.status.idle":"2025-04-03T20:06:19.364889Z","shell.execute_reply.started":"2025-04-03T20:06:15.704181Z","shell.execute_reply":"2025-04-03T20:06:19.364228Z"}},"outputs":[{"name":"stdout","text":"Using default threshold 0.5\n","output_type":"stream"}],"execution_count":13}]}