{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11186856,"sourceType":"datasetVersion","datasetId":6983394}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random, pandas as pd, numpy as np, torch\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\nBATCH_SIZE = 64\nEMB_SIZE = 256\nHIDDEN_SIZE = 512\nNUM_LAYERS = 4\nDROPOUT = 0.4\nNUM_EPOCHS = 50\nLR = 1e-3\nTF_RATIO = 0.4\n\n\ntrain_df = pd.read_csv(\"/kaggle/input/meznarikk/train.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/meznarikk/test.csv\")\n\ndef build_vocab(strings, extra):\n    chars = sorted(set(\"\".join(strings)))\n    itos = extra + chars\n    stoi = {c:i for i,c in enumerate(itos)}\n    return stoi, itos\n\ninp_stoi, inp_itos = build_vocab(train_df.data.tolist()+test_df.data.tolist(), [\"<pad>\",\"<unk>\"])\ntgt_stoi, tgt_itos = build_vocab(train_df.label.tolist(), [\"<pad>\",\"<sos>\",\"<eos>\"])\n\nPAD_INP, UNK = inp_stoi[\"<pad>\"], inp_stoi[\"<unk>\"]\nPAD_TGT, SOS, EOS = tgt_stoi[\"<pad>\"], tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"]\n\nclass DateDataset(Dataset):\n    def __init__(self, df, train=True):\n        self.x = df.data.values\n        self.y = df.label.values if train else None\n\n    def __len__(self): return len(self.x)\n    def __getitem__(self, i):\n        xi = torch.tensor([inp_stoi.get(c, UNK) for c in self.x[i]], dtype=torch.long)\n        if self.y is None:\n            return xi\n        yi = [SOS] + [tgt_stoi[c] for c in self.y[i]] + [EOS]\n        return xi, torch.tensor(yi, dtype=torch.long)\n\ndef collate(batch):\n    if isinstance(batch[0], tuple):\n        xs, ys = zip(*batch)\n        return (\n            nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=PAD_INP),\n            nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=PAD_TGT),\n        )\n    return nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=PAD_INP)\n\n\ntr_df, vl_df = train_test_split(train_df, test_size=0.01, random_state=SEED)\ntrain_loader = DataLoader(DateDataset(tr_df), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\nval_loader   = DataLoader(DateDataset(vl_df), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\ntest_loader  = DataLoader(DateDataset(test_df, train=False), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n\n\nclass Attention(nn.Module):\n    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n        scores = torch.bmm(encoder_outputs, decoder_hidden.unsqueeze(2)).squeeze(2)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn_weights = torch.softmax(scores, dim=1)\n        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n        return context, attn_weights\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_size, hid_size, n_layers, dropout):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_INP)\n        self.rnn = nn.GRU(emb_size, hid_size, num_layers=n_layers, batch_first=True,\n                          bidirectional=True, dropout=dropout)\n\n    def forward(self, src):\n        embedded = self.emb(src)\n        outputs, hidden = self.rnn(embedded)\n        last = torch.cat([hidden[-2], hidden[-1]], dim=1)               \n        hidden = last.unsqueeze(0).repeat(self.rnn.num_layers, 1, 1)    \n        return outputs, hidden\n\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_size, hid_size, n_layers, dropout):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_TGT)\n        self.attn = Attention()\n        self.rnn = nn.GRU(emb_size + hid_size*2, hid_size*2, num_layers=n_layers,\n                          batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hid_size*4, vocab_size)\n\n    def forward(self, token, hidden, encoder_outputs, mask=None):\n        emb = self.emb(token.unsqueeze(1))\n        context, _ = self.attn(hidden[-1], encoder_outputs, mask)\n        rnn_input = torch.cat([emb, context.unsqueeze(1)], dim=2)\n        output, hidden = self.rnn(rnn_input, hidden)\n        logits = self.fc(torch.cat([output.squeeze(1), context], dim=1))\n        return logits, hidden\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, enc, dec):\n        super().__init__()\n        self.enc, self.dec = enc, dec\n\n    def forward(self, src, tgt=None):\n        enc_out, hidden = self.enc(src)\n        max_len = tgt.size(1)-1 if tgt is not None else 10\n        batch = src.size(0)\n        outputs = torch.zeros(batch, max_len, len(tgt_itos), device=DEVICE)\n        token = torch.full((batch,), SOS, device=DEVICE)\n        mask = (src != PAD_INP)\n\n        for t in range(max_len):\n            logits, hidden = self.dec(token, hidden, enc_out, mask)\n            outputs[:,t] = logits\n            token = tgt[:,t+1] if (tgt is not None and random.random()<TF_RATIO) else logits.argmax(1)\n        return outputs\n\nmodel = Seq2Seq(\n    Encoder(len(inp_itos), EMB_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT),\n    Decoder(len(tgt_itos), EMB_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_TGT)\n\n\ndef evaluate(loader):\n    model.eval()\n    loss_sum, correct, total = 0,0,0\n    with torch.no_grad():\n        for x,y in loader:\n            x,y = x.to(DEVICE), y.to(DEVICE)\n            out = model(x,y)\n            loss = criterion(out.view(-1,out.size(-1)), y[:,1:].reshape(-1))\n            loss_sum += loss.item()\n            preds = out.argmax(2)\n            for p,t in zip(preds, y[:,1:]):\n                if \"\".join([tgt_itos[i] for i in p.tolist() if i not in (PAD_TGT,EOS)]) == \\\n                   \"\".join([tgt_itos[i] for i in t.tolist() if i not in (PAD_TGT,EOS)]):\n                    correct+=1\n                total+=1\n    return loss_sum/len(loader), correct/total\n\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    model.train()\n    train_loss = 0\n    for x,y in train_loader:\n        x,y = x.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad()\n        out = model(x,y)\n        loss = criterion(out.view(-1,out.size(-1)), y[:,1:].reshape(-1))\n        loss.backward(); optimizer.step()\n        train_loss += loss.item()\n    val_loss,val_acc = evaluate(val_loader)\n    print(f\"Epoch {epoch}: Train Loss={train_loss/len(train_loader):.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n\nmodel.eval()\npreds=[]\nwith torch.no_grad():\n    for x in test_loader:\n        x = x.to(DEVICE)\n        out = model(x)\n        for seq in out.argmax(2):\n            txt = \"\".join([tgt_itos[i] for i in seq.tolist() if i not in (PAD_TGT,EOS)])\n            preds.append(txt)\n\npd.DataFrame({\"id\":test_df.id, \"label\":preds}).to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T19:57:14.764946Z","iopub.execute_input":"2025-03-27T19:57:14.765236Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss=2.1267, Val Loss=2.5274, Val Acc=0.0000\nEpoch 2: Train Loss=1.7450, Val Loss=1.2810, Val Acc=0.0000\nEpoch 3: Train Loss=0.8682, Val Loss=0.6611, Val Acc=0.0000\nEpoch 4: Train Loss=0.5605, Val Loss=0.5357, Val Acc=0.0000\nEpoch 5: Train Loss=0.4654, Val Loss=0.4906, Val Acc=0.0909\nEpoch 6: Train Loss=0.3563, Val Loss=0.4080, Val Acc=0.0909\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}